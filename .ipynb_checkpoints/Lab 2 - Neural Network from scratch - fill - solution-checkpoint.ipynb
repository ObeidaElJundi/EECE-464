{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks can be intimidating, especially for people new to machine learning. There are several Neural Network libraries in Python (ex. Keras & TensorFlow) abstracting complicated details and computations (ex. matrix multiplications and Gradiend Decent). However, for educational purposes, we will build our own NN from scratch using Numpy only. This will help us break down how exactly NN works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets build this NN:\n",
    "<img src=\"images/NN_with_weights_notations_updated.png\" alt=\"FeedForwardNeuralNetwork\" title=\"FeedForwardNeuralNetwork\" height=\"620\" width=\"420\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42) # to reproduce same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation function\n",
    "The output of each artificial neuron is the sum of inputs times weights passed through an activation function. There are a lot of activation functions, each has its own pros & cons...\n",
    "<img src=\"images/activation_functions.png\" alt=\"activation_functions\" title=\"activation_functions\" height=\"220\" width=\"820\"/>\n",
    "Here, we will be using Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define sigmoid and its derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "    \n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x * (1-x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data\n",
    "<img src=\"images/training_data.png\" alt=\"training_data\" title=\"training_data\" height=\"220\" width=\"420\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 18]\n",
      " [ 6 15]\n",
      " [ 8 12]\n",
      " [10  8]]\n",
      "[[89]\n",
      " [84]\n",
      " [79]\n",
      " [72]]\n"
     ]
    }
   ],
   "source": [
    "# X = (hours sleeping, hours studying), y = score on test\n",
    "x = np.array([[3,18],[6,15],[8,12],[10,8]])  # 4 X 2\n",
    "y = np.array([[89],[84],[79],[72]]) # 4 X 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)\n",
    "Sometimes, the range of values of raw data varies widely. In that case, objective functions will not work properly without normalization. Normalizatoin also helps gradient descent to converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.125       0.75      ]\n",
      " [ 0.25        0.625     ]\n",
      " [ 0.33333333  0.5       ]\n",
      " [ 0.41666667  0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "x = x/ 24\n",
    "y = y/ 100\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize learnable parameters: weights and biases\n",
    "usually as small random values, but 0.5 here to compare with our manual solution. Ignore bias for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w1 = np.ones((2,3)) * 0.5 # 2 X 3 --> 3 neurons, each has two inputs\n",
    "w2 = np.ones((1,3)) * 0.5 # 3 X 1 --> single neuron with three inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.8 # learning rate\n",
    "epochs = np.arange(10000)\n",
    "sse = []\n",
    "\n",
    "# return output of hidden layer & output layer\n",
    "def feed_forward(x, w1, w2):\n",
    "    hidden_output = sigmoid( np.matmul(x,w1) ) # 4 X 3\n",
    "    output = sigmoid( np.matmul(hidden_output,w2) ) # 4 X 1\n",
    "    return hidden_output, output\n",
    "\n",
    "# return gradients of last & hidden layer\n",
    "def backpropagation(x, y, y_hat, hidden_output):\n",
    "    delta = (y-y_hat) * sigmoid_prime(y_hat) # 4 X 1\n",
    "    gradient = np.transpose( np.matmul(delta.T, hidden_output) ) # 3 X 1\n",
    "    \n",
    "    delta2 = delta.dot(w1.T) * sigmoid_prime(hidden_output) # 4 X 3\n",
    "    gradient2 = x.T.dot(delta2) # 2 X 3\n",
    "    \n",
    "    return gradient, gradient2\n",
    "\n",
    "for i in epochs:\n",
    "\n",
    "    # feed forward : calculate y_hat\n",
    "    \n",
    "    \n",
    "    # calculate SSE and append to list\n",
    "\n",
    "    # backpropagation : calculate gradients\n",
    "    \n",
    "    # adjusting weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot SSE during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
