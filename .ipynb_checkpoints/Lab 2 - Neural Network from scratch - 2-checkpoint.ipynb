{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks can be intimidating, especially for people new to machine learning. There are several Neural Network libraries in Python (ex. Keras & TensorFlow) abstracting complicated details and computations (ex. matrix multiplications and Gradiend Decent). However, for educational purposes, we will build our own NN from scratch using Numpy only. This will help us break down how exactly NN works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets build this NN:\n",
    "<img src=\"images/FeedForwardNeuralNetwork.png\" alt=\"jupyter_notebook\" title=\"jupyter_notebook\" height=\"520\" width=\"220\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42) # to reproduce same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation function\n",
    "The output of each artificial neuron is the sum of inputs times weights passed through an activation function. There are a lot of activation functions, each has its own pros & cons...\n",
    "<img src=\"images/activation_functions.png\" alt=\"activation_functions\" title=\"activation_functions\" height=\"220\" width=\"820\"/>\n",
    "Here, we will be using Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define sigmoid and its derivative\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data\n",
    "<img src=\"images/training_data.png\" alt=\"training_data\" title=\"training_data\" height=\"220\" width=\"420\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.  18.]\n",
      " [  6.  15.]\n",
      " [  8.  12.]\n",
      " [ 10.   8.]]\n",
      "[[ 89.]\n",
      " [ 84.]\n",
      " [ 79.]\n",
      " [ 72.]]\n"
     ]
    }
   ],
   "source": [
    "# X = (hours sleeping, hours studying), y = score on test\n",
    "x = np.array(([3, 18], [6, 15], [8, 12], [10,8]), dtype=float) # 4 X 2\n",
    "y = np.array(([89.0], [84.0], [79.0], [72.0]), dtype=float) # 4 X 1\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)\n",
    "Sometimes, the range of values of raw data varies widely. In that case, objective functions will not work properly without normalization. Normalizatoin also helps gradient descent to converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.125       0.75      ]\n",
      " [ 0.25        0.625     ]\n",
      " [ 0.33333333  0.5       ]\n",
      " [ 0.41666667  0.33333333]]\n",
      "[[ 0.89]\n",
      " [ 0.84]\n",
      " [ 0.79]\n",
      " [ 0.72]]\n"
     ]
    }
   ],
   "source": [
    "# normalize input only\n",
    "x /= 24\n",
    "y /= 100\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize learnable parameters: weights and biases\n",
    "usually as small random values, but 0.5 here to compare with our manual solution. Ignore bias for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [ 0.5  0.5  0.5]]\n",
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n"
     ]
    }
   ],
   "source": [
    "w1 = np.ones((2,3)) * 0.5  # 2 X 3 --> 3 neurons, each has two inputs\n",
    "w2 = np.ones((3,1)) * 0.5 # 3 X 1 --> single neuron with three inputs\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_output [[ 0.60766317  0.60766317  0.60766317]\n",
      " [ 0.60766317  0.60766317  0.60766317]\n",
      " [ 0.60268534  0.60268534  0.60268534]\n",
      " [ 0.5926666   0.5926666   0.5926666 ]]\n",
      "output [[ 0.71330594]\n",
      " [ 0.71330594]\n",
      " [ 0.71177656]\n",
      " [ 0.70868374]]\n",
      "delta [[ 0.03613404]\n",
      " [ 0.02590901]\n",
      " [ 0.01604759]\n",
      " [ 0.00233625]]\n",
      "gradient [[ 0.04875754]\n",
      " [ 0.04875754]\n",
      " [ 0.04875754]]\n",
      "delta2 [[ 0.00430733  0.00430733  0.00430733]\n",
      " [ 0.00308847  0.00308847  0.00308847]\n",
      " [ 0.00192134  0.00192134  0.00192134]\n",
      " [ 0.000282    0.000282    0.000282  ]]\n",
      "gradient2 [[ 0.00206848  0.00206848  0.00206848]\n",
      " [ 0.00621546  0.00621546  0.00621546]]\n",
      "w1 [[ 0.50165479  0.50165479  0.50165479]\n",
      " [ 0.50497237  0.50497237  0.50497237]]\n",
      "w2 [[ 0.53900603]\n",
      " [ 0.53900603]\n",
      " [ 0.53900603]]\n",
      "final output: [[ 0.88348392]\n",
      " [ 0.83945802]\n",
      " [ 0.79127815]\n",
      " [ 0.7259749 ]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.8 # learning rate\n",
    "epochs = np.arange(10000)\n",
    "sse = []\n",
    "\n",
    "for i in epochs:\n",
    "\n",
    "    # feed forward\n",
    "    hidden_output = sigmoid( np.matmul(x, w1) ) # 4 X 3\n",
    "    output = sigmoid( np.matmul(hidden_output, w2) ) # 4 X 1\n",
    "    \n",
    "    error = np.sum(0.5 * (y - output)**2)\n",
    "    sse.append(error)\n",
    "\n",
    "    # backpropagation\n",
    "\n",
    "    delta = (y-output) * sigmoid_prime(output) # 4 X 1\n",
    "    #gradient = np.transpose(hidden_output.T.dot( delta )) # 1 X 3\n",
    "    gradient = np.transpose( delta.T.dot( hidden_output ) ) # 3 X 1\n",
    "    #w2 += lr * gradient # adjusting second set (hidden --> output) weights\n",
    "\n",
    "    delta2 = delta.dot(w2.T) * sigmoid_prime(hidden_output) # 4 X 3\n",
    "    gradient2 = x.T.dot(delta2) # 2 X 3\n",
    "    \n",
    "    w2 += lr * gradient # adjusting second set (hidden --> output) weights\n",
    "    w1 += lr * gradient2 # adjusting first set (input --> hidden) weights\n",
    "    \n",
    "    if i == 0:\n",
    "        print('hidden_output', hidden_output)\n",
    "        print('output', output)\n",
    "        print('delta', delta)\n",
    "        print('gradient', gradient)\n",
    "        print('delta2', delta2)\n",
    "        print('gradient2', gradient2)\n",
    "        print('w1', w1)\n",
    "        print('w2', w2)\n",
    "\n",
    "#print('w1:', w1, 'w2:', w2)\n",
    "print('final output:', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xaced9a6f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sse)\n",
    "plt.ylabel('SSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         x1        x2     y        h1        h2        h3     y_hat       sse  \\\n",
      "0  0.125000  0.750000  0.89  0.607663  0.607663  0.607663  0.713306  0.015610   \n",
      "1  0.250000  0.625000  0.84  0.607663  0.607663  0.607663  0.713306  0.008026   \n",
      "2  0.333333  0.500000  0.79  0.602685  0.602685  0.602685  0.711777  0.003059   \n",
      "3  0.416667  0.333333  0.72  0.592667  0.592667  0.592667  0.708684  0.000064   \n",
      "\n",
      "      delta  \n",
      "0  0.036134  \n",
      "1  0.025909  \n",
      "2  0.016048  \n",
      "3  0.002336  \n",
      "2.4106782776\n",
      "3.24\n",
      "2.84707217613\n",
      "0.0804268932773\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sse(x1,x2,w1,w2,y):\n",
    "    y_hat = w1*x1 + w2*x2\n",
    "    print(y_hat)\n",
    "    sse = 0.5 * (y - y_hat) **2\n",
    "    print(sse)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['x1'] = x[:,0]\n",
    "df['x2'] = x[:,1]\n",
    "df['y'] = y.flatten()\n",
    "df['h1'] = hidden_output[:,0]\n",
    "df['h2'] = hidden_output[:,1]\n",
    "df['h3'] = hidden_output[:,2]\n",
    "df['y_hat'] = output.flatten()\n",
    "df['sse'] = 0.5 * (df['y'] - df['y_hat']) **2\n",
    "df['delta'] = (df['y'] - df['y_hat']) * sigmoid_prime(df['y_hat']) # 4 X 1\n",
    "#df['Gw7'] = df['delta'] * df['h1']\n",
    "#df['Gw8'] = df['delta'] * df['h2']\n",
    "#df['Gw9'] = df['delta'] * df['h3']\n",
    "print(df)\n",
    "print(df['h1'].sum())\n",
    "print(df['y'].sum())\n",
    "print(df['y_hat'].sum())\n",
    "print(df['delta'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00525310854144 0.00525310854144 0.00525310854144\n",
      "0.00357471230143 0.00357471230143 0.00357471230143\n",
      "0.00210716972117 0.00210716972117 0.00210716972117\n",
      "0.000285905501693 0.000285905501693 0.000285905501693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hours sleeping</th>\n",
       "      <th>hours studing</th>\n",
       "      <th>final grade</th>\n",
       "      <th>h1</th>\n",
       "      <th>h2</th>\n",
       "      <th>h3</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>SSE</th>\n",
       "      <th>delta</th>\n",
       "      <th>gradient 7</th>\n",
       "      <th>gradient 8</th>\n",
       "      <th>gradient 9</th>\n",
       "      <th>gradient 1</th>\n",
       "      <th>gradient 2</th>\n",
       "      <th>gradient 3</th>\n",
       "      <th>gradient 4</th>\n",
       "      <th>gradient 5</th>\n",
       "      <th>gradient 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.713306</td>\n",
       "      <td>0.015610</td>\n",
       "      <td>0.036134</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.021957</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.003940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.607663</td>\n",
       "      <td>0.713306</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.025909</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.015744</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.002234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.602685</td>\n",
       "      <td>0.602685</td>\n",
       "      <td>0.602685</td>\n",
       "      <td>0.711777</td>\n",
       "      <td>0.003059</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>0.009672</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.592667</td>\n",
       "      <td>0.592667</td>\n",
       "      <td>0.592667</td>\n",
       "      <td>0.708684</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hours sleeping  hours studing  final grade        h1        h2        h3  \\\n",
       "0        0.125000       0.750000         0.89  0.607663  0.607663  0.607663   \n",
       "1        0.250000       0.625000         0.84  0.607663  0.607663  0.607663   \n",
       "2        0.333333       0.500000         0.79  0.602685  0.602685  0.602685   \n",
       "3        0.416667       0.333333         0.72  0.592667  0.592667  0.592667   \n",
       "\n",
       "      y_hat       SSE     delta  gradient 7  gradient 8  gradient 9  \\\n",
       "0  0.713306  0.015610  0.036134    0.021957    0.021957    0.021957   \n",
       "1  0.713306  0.008026  0.025909    0.015744    0.015744    0.015744   \n",
       "2  0.711777  0.003059  0.016048    0.009672    0.009672    0.009672   \n",
       "3  0.708684  0.000064  0.002336    0.001385    0.001385    0.001385   \n",
       "\n",
       "   gradient 1  gradient 2  gradient 3  gradient 4  gradient 5  gradient 6  \n",
       "0    0.000657    0.003940    0.000657    0.003940    0.000657    0.003940  \n",
       "1    0.000894    0.002234    0.000894    0.002234    0.000894    0.002234  \n",
       "2    0.000702    0.001054    0.000702    0.001054    0.000702    0.001054  \n",
       "3    0.000119    0.000095    0.000119    0.000095    0.000119    0.000095  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x = np.array(([3, 18], [6, 15], [8, 12], [10,8]), dtype=float) # 4 X 2\n",
    "y = np.array(([89], [84], [79], [72]), dtype=float) # 4 X 1\n",
    "\n",
    "x /= 24\n",
    "y /= 100\n",
    "\n",
    "w1 = np.ones((3,2)) * 0.5  # 3 X 2 --> 3 neurons, each has two inputs\n",
    "w2 = np.ones((1,3)) * 0.5 # 1 X 3 --> single neuron with three inputs\n",
    "\n",
    "h1s, h2s, h3s, y_hats, y_y_hats, sses, deltas, G7s, G8s, G9s = [], [], [], [], [], [], [], [], [], []\n",
    "G1s, G2s, G3s, G4s, G5s, G6s = [], [], [], [], [], []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(x)):\n",
    "    # ff\n",
    "    h1 = sigmoid( x[i,0] * w1[0,0] + x[i,1] * w1[0,1] )\n",
    "    h1s.append(h1)\n",
    "    h2 = sigmoid( x[i,0] * w1[1,0] + x[i,1] * w1[1,1] )\n",
    "    h2s.append(h2)\n",
    "    h3 = sigmoid( x[i,0] * w1[2,0] + x[i,1] * w1[2,1] )\n",
    "    h3s.append(h3)\n",
    "    y_hat = sigmoid( h1 * w2[0,0] + h2 * w2[0,1] + h1 * w2[0,2] )\n",
    "    y_hats.append(y_hat)\n",
    "    sse = 0.5 * (y[i,0] - y_hat)**2\n",
    "    sses.append(sse)\n",
    "    y_y_hats.append(y[i,0] - y_hat)\n",
    "    # Bp\n",
    "    delta = (y[i,0] - y_hat) * y_hat * (1 - y_hat)\n",
    "    deltas.append(delta)\n",
    "    G7 = delta * h1\n",
    "    G8 = delta * h2\n",
    "    G9 = delta * h3\n",
    "    G7s.append(G7)\n",
    "    G8s.append(G8)\n",
    "    G9s.append(G9)\n",
    "    \n",
    "    ####  update w2 || calculate Gradiends 1 to 9.. whuch is first ???\n",
    "    \n",
    "    #update weights\n",
    "#     w2_new1 = w2[0,0] + 5*G7\n",
    "#     w2_new2 = w2[0,1] + 5*G8\n",
    "#     w2_new3 = w2[0,2] + 5*G9\n",
    "    \n",
    "    delta2 = delta * h1 * (1-h1) * w2_new1\n",
    "    delta3 = delta * h2 * (1-h2) * w2_new2\n",
    "    delta4 = delta * h3 * (1-h3) * w2_new3\n",
    "    print(delta2, delta3, delta4)\n",
    "    G1 = delta2 * x[i,0]\n",
    "    G2 = delta2 * x[i,1]\n",
    "    G3 = delta3 * x[i,0]\n",
    "    G4 = delta3 * x[i,1]\n",
    "    G5 = delta4 * x[i,0]\n",
    "    G6 = delta4 * x[i,1]\n",
    "    G1s.append(G1)\n",
    "    G2s.append(G2)\n",
    "    G3s.append(G3)\n",
    "    G4s.append(G4)\n",
    "    G5s.append(G5)\n",
    "    G6s.append(G6)\n",
    "    \n",
    "\n",
    "df['hours sleeping'] = x[:,0]\n",
    "df['hours studing'] = x[:,1]\n",
    "df['final grade'] = y[:,0]\n",
    "df['h1'] = h1s\n",
    "df['h2'] = h2s\n",
    "df['h3'] = h3s\n",
    "df['y_hat'] = y_hats\n",
    "df['SSE'] = sses\n",
    "df['delta'] = deltas\n",
    "df['gradient 7'] = G7s\n",
    "df['gradient 8'] = G8s\n",
    "df['gradient 9'] = G9s\n",
    "\n",
    "df['gradient 1'] = G1s\n",
    "df['gradient 2'] = G2s\n",
    "df['gradient 3'] = G3s\n",
    "df['gradient 4'] = G4s\n",
    "df['gradient 5'] = G5s\n",
    "df['gradient 6'] = G6s\n",
    "\n",
    "df.head()\n",
    "\n",
    "#print(sum(h1s), sum(h2s), sum(h3s), sum(y_hats), sum(y_y_hats), sum(sses), sum(deltas), sum(G7s), sum(G8s), sum(G9s))\n",
    "#print(h1, h2, h3, y_hat)\n",
    "\n",
    "# Bp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('backpropagation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002371833842465075"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gradient 1'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.531075"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5+5*0.006215"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
